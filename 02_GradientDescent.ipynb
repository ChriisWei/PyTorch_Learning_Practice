{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0  #设定初始值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(xs, ys):\n",
    "    cost = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        y_pred = forward(x)\n",
    "        cost += (y_pred - y) ** 2\n",
    "    return cost / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(xs, ys):\n",
    "    grad = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        grad += 2 * x * (x * w - y)\n",
    "    return grad / len(xs)\n",
    "#此处gradient的计算公式由前文MSE公式求导得到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = []\n",
    "loss_list = []\n",
    "\n",
    "print('Predict (before training)', 4, forward(4))\n",
    "for epoch in range(100):\n",
    "    cost_val = cost(x_data, y_data)\n",
    "    grad_val = gradient(x_data, y_data)     #gradient是与w相关的，每迭代出一个新的w后重新对gradient进行计算\n",
    "    w -= 0.01 * grad_val                    #设定0.01为学习率，对w进行迭代\n",
    "    print('Epoch:', epoch, 'w=', w, 'loss=', cost_val)\n",
    "    epoch_list.append(epoch)\n",
    "    loss_list.append(cost_val)\n",
    "print('Predict (after training)', 4, forward(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGC (Stochastic Gradient Descent)**\n",
    "\n",
    "随机梯度下降SGD不使用整个dataset计算gradient, 而是使用一个随机样本计算\n",
    "这样做与一般的Gradient Descent有所不同：\n",
    "\n",
    "*收敛速度*： 对于大型数据集，SGD 的收敛速度通常比批量梯度下降快得多，因为它更新权重的频率更高\n",
    "\n",
    "*噪声的引入*： SGD 将噪声引入学习过程，这有助于逃避局部最小值（以及鞍点），但也可能意味着损失函数更加不稳定，可能无法收敛到精确的最小值。\n",
    "\n",
    "下方代码框给出了改为SGD的迭代代码，其中gradient函数与之前发生了变化\n",
    "\n",
    "**然而，只选一个变量进行计算时间复杂度是很差的，而用整个dataset进行计算性能太低，所以人们提出了一个折中的方案，叫做batch，即把dataset分为很多的小块，这是目前SGD的默认方案**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(100):\n",
    "#     for i in range(len(x_data)):\n",
    "#         x = x_data[i]\n",
    "#         y = y_data[i]\n",
    "#         # Compute the gradient for this single data point or batch\n",
    "#         grad_val = gradient(x, y)     #gradient的参数类型由list变为了单一数据\n",
    "#         w -= 0.01 * grad_val\n",
    "#         # Optionally, record the cost and update the epoch and loss lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_list, loss_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()  #网格线\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
